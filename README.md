# llm-chat-bot
Multimodality Pre-trained large models
This project uses a combination of image pre-training model and LLM to become a multimodal LLM, which can answer questions that combine images and text. As the development leader, I first read and reproduced the VIT+GPT method in the paper Pan
Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan The
36th Conference on Neural Information Processing Systems (NeurIPS), 2022. After that, the project team created a BLIP model to replace the relatively backward VIT, generate more detailed text descriptions and transmit them to GPT. The study found that the accuracy rate was greatly improved (3%).
